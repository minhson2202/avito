{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3f1c8-418a-438e-b814-4e1e990a14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4552 unique locations from XML.\n",
      "Loaded 54 business areas from XML.\n",
      "Total tasks: 245808, Chunk size: 8194, Number of chunks: 30\n",
      "[349767] Using proxy: http://92.63.77.130:3139 | Params: {'locations': '637640', 'business_area': '3278315', 'per_page': 100, 'page': 1}[349768] Using proxy: http://92.63.77.224:3139 | Params: {'locations': '72', 'business_area': '3278987', 'per_page': 100, 'page': 1}[349787] Using proxy: http://92.63.77.200:3139 | Params: {'locations': '622070', 'business_area': '3278359', 'per_page': 100, 'page': 1}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "from itertools import cycle\n",
    "\n",
    "# Define proxies\n",
    "proxies = [f'http://92.63.77.{ip}:3139' for ip in range(130, 230)]\n",
    "\n",
    "def fetch_vacancies_with_session(session, url, headers, params, proxy_dict):\n",
    "    max_retries = 10  \n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        print(f\"[{os.getpid()}] Using proxy: {proxy_dict['http']} | Params: {params}\")\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, params=params, proxies=proxy_dict, timeout=10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:\n",
    "                retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "                print(f\"[{os.getpid()}] Rate limit (429). Retrying after {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"[{os.getpid()}] Error with proxy {proxy_dict['http']}: {e}\")\n",
    "            retry_count += 1\n",
    "        time.sleep(1)\n",
    "    return None\n",
    "\n",
    "        \n",
    "def process_chunk(chunk_tasks, access_token, url, headers, chunk_index):\n",
    "    aggregated_vacancies = []\n",
    "    with requests.Session() as session:\n",
    "        for location, business_area, proxy in chunk_tasks:\n",
    "            proxy_dict = {\"http\": proxy, \"https\": proxy}  # Assign task-specific proxy\n",
    "            current_page = 1\n",
    "            while True:\n",
    "                params = {\n",
    "                    \"locations\": location[\"id\"],\n",
    "                    \"business_area\": business_area[\"id\"],\n",
    "                    \"per_page\": 100,\n",
    "                    \"page\": current_page,\n",
    "                }\n",
    "                try:\n",
    "                    data = fetch_vacancies_with_session(session, url, headers, params, proxy_dict)\n",
    "                    if data:\n",
    "                        vacancies = data.get(\"vacancies\", [])\n",
    "                        aggregated_vacancies.extend(vacancies)\n",
    "                        \n",
    "                        meta = data.get(\"meta\", {})\n",
    "                        total_pages = meta.get(\"pages\", 1)\n",
    "                        if current_page >= total_pages:\n",
    "                            break\n",
    "                        current_page += 1\n",
    "                    else:\n",
    "                        break  \n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching vacancies for location {location['name']} and business area {business_area['name']}: {e}\")\n",
    "                    break\n",
    "\n",
    "    filename = f\"vacancies_chunk_{chunk_index}.csv\"\n",
    "    if aggregated_vacancies:\n",
    "        df = pd.DataFrame(aggregated_vacancies)\n",
    "        print(f\"Writing {len(df)} vacancies to {filename}\")\n",
    "        df.to_csv(filename, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    else:\n",
    "        print(f\"No vacancies to write for chunk {chunk_index}.\")\n",
    "\n",
    "    return aggregated_vacancies\n",
    "\n",
    "def process_vacancies_multiprocessing(access_token, locations, business_areas, url, headers, processes=10):\n",
    "    tasks = [\n",
    "        (location, business_area, proxy) \n",
    "        for (location, business_area), proxy in zip(\n",
    "            [(loc, ba) for loc in locations for ba in business_areas], cycle(proxies)\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    total_tasks = len(tasks)\n",
    "    chunk_size = ceil(total_tasks / processes)\n",
    "    chunks = [tasks[i:i+chunk_size] for i in range(0, total_tasks, chunk_size)]\n",
    "\n",
    "    print(f\"Total tasks: {total_tasks}, Chunk size: {chunk_size}, Number of chunks: {len(chunks)}\")\n",
    "\n",
    "    with mp.Pool(processes=processes) as pool:\n",
    "        results = pool.starmap(process_chunk, [(chunk, access_token, url, headers, idx) for idx, chunk in enumerate(chunks, start=1)])\n",
    "    \n",
    "    aggregated_results = []\n",
    "    for result in results:\n",
    "        aggregated_results.extend(result)\n",
    "    \n",
    "    print(f\"Total aggregated vacancies fetched: {len(aggregated_results)}\")\n",
    "    return aggregated_results\n",
    "\n",
    "\n",
    "def get_access_token():\n",
    "    url = \"https://api.avito.ru/token\"\n",
    "    params = {\n",
    "        \"client_id\": \"oNwJeKq7XxKdbMisWAw7\",\n",
    "        \"client_secret\": \"wsFicRL8q2lmfPnYMcevaVyf9kwnAV7QNdU-Jjtd\",\n",
    "        \"grant_type\": \"client_credentials\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"access_token\")\n",
    "        else:\n",
    "            print(f\"Failed to get access token. Status Code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while getting access token: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_locations_from_xml(xml_file):\n",
    "    locations = []\n",
    "    try:\n",
    "        with open(xml_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'xml')\n",
    "\n",
    "            for region in soup.find_all('Region'):\n",
    "                region_id = region.get('Id')\n",
    "                region_name = region.get('Name')\n",
    "\n",
    "                if region_id and region_name:\n",
    "                    locations.append({\"id\": region_id, \"name\": region_name, \"type\": \"Region\"})\n",
    "\n",
    "                    for dicroad in region.find_all('DirectionRoad'):\n",
    "                        dicroad_id = dicroad.get('Id')\n",
    "                        dicroad_name = dicroad.get('Name')\n",
    "                        if dicroad_id and dicroad_name:\n",
    "                            locations.append({\"id\": dicroad_id, \"name\": f\"{dicroad_name} ({region_name})\", \"type\": \"DirectionRoad\"})\n",
    "\n",
    "                    for subway in region.find_all('Subway'):\n",
    "                        subway_id = subway.get('Id')\n",
    "                        subway_name = subway.get('Name')\n",
    "                        if subway_id and subway_name:\n",
    "                            locations.append({\"id\": subway_id, \"name\": f\"{subway_name} ({region_name})\", \"type\": \"Subway\"})\n",
    "\n",
    "                    for city in region.find_all('City'):\n",
    "                        city_id = city.get('Id')\n",
    "                        city_name = city.get('Name')\n",
    "                        if city_id and city_name:\n",
    "                            locations.append({\"id\": city_id, \"name\": f\"{city_name} ({region_name})\", \"type\": \"City\"})\n",
    "\n",
    "                            for district in city.find_all('District'):\n",
    "                                district_id = district.get('Id')\n",
    "                                district_name = district.get('Name')\n",
    "                                if district_id and district_name:\n",
    "                                    locations.append({\"id\": district_id, \"name\": f\"{district_name} ({city_name})\", \"type\": \"District\"})\n",
    "\n",
    "        print(f\"Loaded {len(locations)} unique locations from XML.\")\n",
    "        return locations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading locations from XML: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_business_areas_from_xml(xml_file):\n",
    "    business_areas = []\n",
    "    try:\n",
    "        with open(xml_file, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'xml')\n",
    "            for business_area in soup.find_all('BusinessArea'):\n",
    "                area_id = business_area.find('id').text if business_area.find('id') else None\n",
    "                area_name = business_area.find('name').text if business_area.find('name') else None\n",
    "                if area_id and area_name:\n",
    "                    business_areas.append({\"id\": area_id, \"name\": area_name})\n",
    "        \n",
    "        print(f\"Loaded {len(business_areas)} business areas from XML.\")\n",
    "        return business_areas\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading business areas from XML: {e}\")\n",
    "        return []\n",
    "        \n",
    "def init_process(proxy_list):\n",
    "    global process_proxy_cycle\n",
    "    process_proxy_cycle = cycle(proxy_list)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    access_token = get_access_token()\n",
    "    start_time = time.time()  \n",
    "\n",
    "    if access_token:\n",
    "        locations = load_locations_from_xml(\"catalog-location.xml\")\n",
    "        business_areas = load_business_areas_from_xml(\"catalog-business-area.xml\")\n",
    "        \n",
    "        url = \"https://api.avito.ru/job/v2/vacancies\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {access_token}\"\n",
    "        }\n",
    "        \n",
    "        all_vacancies = process_vacancies_multiprocessing(\n",
    "            access_token,\n",
    "            locations=locations,\n",
    "            business_areas=business_areas,\n",
    "            url=url,\n",
    "            headers=headers,\n",
    "            processes=30  \n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Total execution time: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Total aggregated vacancies fetched: {len(all_vacancies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51922c6b-9fe1-4351-80c2-03ae6095b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e46bd-236b-4ad1-9957-de42f5962709",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d7231-cbcb-4ad7-8ef8-f18e2803c286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
